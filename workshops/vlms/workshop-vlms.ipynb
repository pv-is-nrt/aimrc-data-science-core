{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Programmatic querying of large (vision) language models for research with focus on scientific images.\n",
        "\n",
        "> Author: Prateek Verma  \n",
        "> Notebook created for Data Science Core Workshop Series, AIMRC @ U Arkansas  \n",
        "> This Notebook is designed to run entirely in Google Colab.\n",
        "\n",
        "Welcome to the Large (Vision) Language Models Workshop! In this workshop, we will learn about some of the state-of-the art large language models and large vision language models and how to use them programmatically. This will also be a hands-on workshop like the previous ones where we will go through a variety of models to perform tasks like captioning, classification, segmentation, and so on. We hope that after this workshop you will be able to think of ideas to leverage and incorporate the ever-increasing power of LLMs into your own research. This workshop is beginner friendly."
      ],
      "metadata": {
        "id": "RyhKmdVVSPB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you begin\n",
        "\n",
        "While I highly recommend to use your own images during the exercises, you are welcome to (also) use the ones I am using. You can download them from our [GitHub repo](https://github.com/pv-is-nrt/aimrc-data-science-core/blob/main/workshops/vlms/sample_images.zip). Be sure to unzip the file after downloading."
      ],
      "metadata": {
        "id": "LevbXHLMM6oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Image Captioning with BLIP\n",
        "\n",
        "Learn how to use the BLIP (Bootstrapped Language-Image Pretraining) model to automatically generate captions for images. Image captioning is the task of generating descriptive text for a given image, which can be used in various applications such as:\n",
        "\n",
        "- Enhancing accessibility for visually impaired users.\n",
        "- Automatic description generation for images on social media.\n",
        "- Supporting content creators by generating captions for images quickly.\n",
        "\n",
        "We will use a pre-trained version of the BLIP model from Hugging Face, which is designed to understand the visual content of an image and generate meaningful descriptions based on that content. You will also have the opportunity to experiment with adding prompts to influence the generated captions."
      ],
      "metadata": {
        "id": "XMC8hdptwGjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install and import necessary libraries\n",
        "\n",
        "The necessary libraries come preinstalled in Google Colab, but if you want to run this on your own computer or a server, you will need to install these libraries first. A good approach is to create a new virtual environment (whether Python or Conda) for each model."
      ],
      "metadata": {
        "id": "hU3OoAIrwZ1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QSbsABDlu7-"
      },
      "outputs": [],
      "source": [
        "# these should come preinstalled in a Google Colab environment\n",
        "#!pip install transformers\n",
        "#!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "4gq7KK5ql_od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load the pre-trained BLIP model and processor from Hugging Face\n",
        "\n",
        "This will download about 1 GB data temporarily to your disk. This includes a copy of the pretrained language model. Always check out the model's website or GitHub page to learn how to use them."
      ],
      "metadata": {
        "id": "Yyf47w7qw7ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "id": "mEMo0QEbmBKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create some functions\n",
        "\n",
        "Note that the second function makes use of the `processor` and `model` objects created above. The first function uses standard Python library to make uploading of images possible."
      ],
      "metadata": {
        "id": "bJIuv_1cxBt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to upload and preprocess image\n",
        "def upload_image():\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        image = Image.open(fn).convert('RGB')\n",
        "        return image"
      ],
      "metadata": {
        "id": "2HptB-qUmC1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the caption generation\n",
        "def generate_caption(image):\n",
        "    # Preprocess the image\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    # Generate caption\n",
        "    output = model.generate(**inputs, max_length=100)\n",
        "    # Decode and return the generated caption\n",
        "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "    return caption"
      ],
      "metadata": {
        "id": "9W5iWwCSmFeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Upload the image and generate caption"
      ],
      "metadata": {
        "id": "j4Xfs0kLxIsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please upload an image:\")\n",
        "image = upload_image()"
      ],
      "metadata": {
        "id": "vLcSH6pEmGvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and display the caption\n",
        "caption = generate_caption(image)\n",
        "print(\"Generated Caption:\", caption)"
      ],
      "metadata": {
        "id": "a81CHTUfmI9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Very fun homework:\n",
        "\n",
        "\n",
        "1.   Modify the code so that instead of asking the user to upload the image, the user is able to specify a path to the image stored on user's Google Drive.\n",
        "2.   Use a for loop to loop through all images in a directory and generate captions for all of them.\n",
        "3. Save a csv file with file paths and captions for each image.\n",
        "4. BLIP can answer questions about the image for you. You can send a prompt and generate an output. You will need to use the BLIP model variant that can handle VQA tasks (note that in this exercise you used the captioning variant). Try it out.\n",
        "```Python\n",
        "  # Load the pre-trained BLIP model and processor for VQA from Hugging Face\n",
        "  processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "  model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "  # BLIP with VQA prompt\n",
        "  prompt = \"Is this picture of outdoors or indoors?\"\n",
        "  # Preprocess the image and prompt for VQA mode\n",
        "  inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
        "  # Generate the answer with a limited output length\n",
        "  output = model.generate(**inputs, max_length=20)\n",
        "  # Decode and return the generated answer\n",
        "  response = processor.decode(output[0], skip_special_tokens=True)\n",
        "  print(f\"Generated Response: {response}\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "guGn-NQSoDwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Image Classification with CLIP\n",
        "\n",
        "In this exercise, you'll learn how to use the CLIP (Contrastive Language-Image Pretraining) model to classify images based on provided text labels. CLIP is a powerful model that aligns visual data (images) with textual data (text prompts). By comparing an image with multiple possible labels, CLIP determines which label best matches the content of the image.\n",
        "\n",
        "You'll upload an image and provide a list of possible answers. CLIP will return the label that best describes the image, showcasing its ability to perform zero-shot classification without prior training on specific categories."
      ],
      "metadata": {
        "id": "3J_p-oMJyUGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install and import necessary libraries\n",
        "\n",
        "We do not need to install the libraries again (already available in Google Colab), and also because they were taken care of in the previous exercise."
      ],
      "metadata": {
        "id": "qjiA_sSRYz4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "Bs_VenrNynFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load the pre-trained CLIP model and processor from Hugging Face\n",
        "This will download about 600 + 400 MB of data temporarily to your disk. This includes a copy of the pretrained language model."
      ],
      "metadata": {
        "id": "8yY2OTLwD9yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CLIP model and processor from Hugging Face\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "JC0XtOJcyrLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create some functions\n",
        "\n",
        "The output generation function may differ greatly from model to model and we usually have to refer to the maker's website to learn how to implement them. However, the semantics and features are similar across various language models that you'll get a hang of with experience."
      ],
      "metadata": {
        "id": "fnt9lzPoEUqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to upload and preprocess image\n",
        "def upload_image():\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        image = Image.open(fn).convert('RGB')\n",
        "        return image\n",
        "\n",
        "# Function to generate answers based on image and text prompts\n",
        "def generate_answer(image, possible_answers):\n",
        "    inputs = processor(text=possible_answers, images=image, return_tensors=\"pt\", padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)  # Convert logits to probabilities\n",
        "    best_idx = probs.argmax().item()  # Get the index of the highest probability\n",
        "    return possible_answers[best_idx]  # Return the answer with the highest score"
      ],
      "metadata": {
        "id": "skXGZUydATYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Upload image, ask a question, provide possible answers, and generate the right answer\n",
        "\n",
        "In the CLIP classification step, you upload an image, ask a question about it, and provide possible answers to guide the model in selecting the most relevant label. The provided answers act as options for the model to compare with the image, enabling it to choose the best match based on visual-text alignment. This is important because CLIP doesn’t generate answers on its own but works by ranking pre-defined options.\n",
        "\n",
        "Beyond classification, CLIP can perform tasks like image captioning, visual search, and even zero-shot learning, allowing it to generalize well across unseen data without additional training."
      ],
      "metadata": {
        "id": "uNHuryB7E6Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload an image and ask a question\n",
        "print(\"Please upload an image:\")\n",
        "image = upload_image()\n",
        "\n",
        "# Ask a question about the image\n",
        "question = \"Is this image taken indoors or outdoors?\"\n",
        "\n",
        "# Provide possible answers\n",
        "possible_answers = [\"outdoors\", \"indoors\", \"unclear\"]\n",
        "answer = generate_answer(image, possible_answers)\n",
        "\n",
        "print(f\"Generated Answer: {answer}\")"
      ],
      "metadata": {
        "id": "Y4AvXJXyAotk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Object isolation using Segment Anything Model\n",
        "\n",
        "In this exercise, you will use the Segment Anything Model (SAM) to perform image segmentation, a task where objects within an image are identified and separated into different regions. SAM is a versatile model designed to handle a wide range of images without requiring task-specific training. You'll upload an image, and SAM will automatically generate segmentation masks that highlight the various objects present in the image.\n",
        "\n",
        "By the end of this exercise, you will understand how SAM processes images and generates segmentation masks, making it a powerful tool for tasks like object detection, medical imaging, and visual analysis.\n",
        "\n",
        "NOTE: I advise you to switch to a GPU kernel for this exercise.  \n",
        "`Runtime` > `Change runtime type` > `T4 GPU`"
      ],
      "metadata": {
        "id": "zKtQg9GaEaU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Clone SAM repository and install dependencies\n",
        "\n",
        "Because SAM is not served via HuggingFace, we clone its repo and go from there. Once cloned, we install the `segment-anything` library'', so we can use it."
      ],
      "metadata": {
        "id": "A4UbbOVrOMSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/segment-anything.git\n",
        "%cd segment-anything\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "xM0a2vwtEfkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Install and import other necessary libraries"
      ],
      "metadata": {
        "id": "KYPNrsMRQlZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional dependencies (these too should already be installed in Colab)\n",
        "# !pip install opencv-python-headless matplotlib"
      ],
      "metadata": {
        "id": "iYSI-y9MEopG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "ulx1gxOGFCg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Download the pretrained SAM model\n",
        "\n",
        "And specify SAM to use this model. SAM weighs about 200 MBs."
      ],
      "metadata": {
        "id": "dOVh395kQz4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the pre-trained SAM model\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O sam_vit_h.pth"
      ],
      "metadata": {
        "id": "-Pb766tbFHCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Specify the path to the model to SAM\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h.pth\").to(device)\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ],
      "metadata": {
        "id": "s_VGi4OnGBym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Upload image and run SAM\n",
        "\n",
        "We create function for uploading image just like before. Because SAM runs slower on larger images, we also include code to automatically resize the image. We also copy a function from SAM's website that helps us to show an overlay of the generated masks on top of the input image."
      ],
      "metadata": {
        "id": "mVcguvPjRj1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload and resize the image if needed\n",
        "def upload_and_resize_image(target_width=512):\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        image = cv2.imread(fn)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for correct visualization\n",
        "        # Get the original dimensions of the image\n",
        "        height, width = image.shape[:2]\n",
        "        if width > 500:\n",
        "          # Calculate the new height while maintaining the aspect ratio\n",
        "          aspect_ratio = height / width\n",
        "          new_height = int(target_width * aspect_ratio)\n",
        "          # Resize the image while maintaining the aspect ratio\n",
        "          image = cv2.resize(image, (target_width, new_height))\n",
        "        return image\n",
        "\n",
        "# Show mask overlays on top of images (copied from SAM's website)\n",
        "def show_anns(anns):\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "\n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)"
      ],
      "metadata": {
        "id": "lohpOr_kGVfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating masks takes up to 10 minutes on a CPU and 20 s on a GPU runtime."
      ],
      "metadata": {
        "id": "fYJRabBzl8-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please upload an image:\")\n",
        "image = upload_and_resize_image()\n",
        "\n",
        "# Use SAM to generate segmentation masks\n",
        "print(\"Generating segmentation mask...\")\n",
        "masks = mask_generator.generate(image)"
      ],
      "metadata": {
        "id": "ZSxIQ22zGYqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the original image and segmentation mask\n",
        "plt.figure()\n",
        "plt.imshow(image)\n",
        "show_anns(masks)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZcAdstuzYqwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Some analysis on masks\n",
        "\n",
        "You can do some analysis on masks with help of properties of the generated mask objects. Use SAM's website for reference. Here we have accessed the area and bounding box properties of the masks."
      ],
      "metadata": {
        "id": "vNn_bAOlqBMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(image)\n",
        "show_anns(masks)\n",
        "\n",
        "image_area = image.shape[0] * image.shape[1]\n",
        "print(f\"Image area: {image_area}\")\n",
        "\n",
        "# add text with mask number on top of the image\n",
        "for i in range(len(masks)):\n",
        "    plt.text(\n",
        "        masks[i]['bbox'][0] + masks[i]['bbox'][3]/2, # x-coordinate\n",
        "        masks[i]['bbox'][1] + masks[i]['bbox'][2]/2, # y-coordinate\n",
        "        str(i),\n",
        "        fontsize=8,\n",
        "        color='white',\n",
        "        va='center',\n",
        "        ha='center',\n",
        "        bbox=dict(boxstyle='round', facecolor='black', alpha=0.5)\n",
        "    )\n",
        "\n",
        "    # you can also print the fraction of the image area that the mask occupies\n",
        "    mask_area = masks[i]['area']\n",
        "    fraction_area = np.round(mask_area * 100 / image_area, 2)\n",
        "    print(\"mask\", str(i), \"area:\", mask_area, \"\\tfraction:\", fraction_area)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YYBCG92Vm2Bb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}