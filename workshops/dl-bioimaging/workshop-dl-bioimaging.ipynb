{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1hX298IbMTj"
      },
      "source": [
        "# Deep Learning in Bioimaging Workshop\n",
        "\n",
        "> Author: Prateek Verma  \n",
        "> Notebook created for Data Science Core Workshop Series, AIMRC @ U Arkansas  \n",
        "> This Notebook is designed to run in Google Colab.\n",
        "\n",
        "Welcome to the Deep Learning in Bioimaging Workshop! In this workshop, we will explore various deep learning techniques and apply them to biomedical images using Fiji (ImageJ) and Google Colab. This workshop is designed to be executed by beginners, so don't worry if you are new to deep learning or bioimaging. Even if you don't understand the code, you will get familiar with the concepts and the workflow for applying state-of-the art deep learning techniques to bioimaging.\n",
        "\n",
        "We will cover the following exercises:\n",
        "1. Image Classification (CNN, Google Colab)\n",
        "2. Image Segmentation (Fiji)\n",
        "3. Image Segmentation (U-Net, Google Colab)\n",
        "4. Image Denoising (ZeroCostDL4Mic, Google Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-workshop preparation\n",
        "\n",
        "Steps to upload denoising dataset to your Google Drive.\n",
        "1. Click [this link](https://zenodo.org/records/5750174/files/Denoising_Dataset.zip?download=1) to download the Denoising dataset to your local computer.\n",
        "2. Extract the downloaded zip file to your local computer.\n",
        "3. In your Google Drive page in your browser, go to `+ New > Folder upload` and upload the extracted `Denoising_Dataset` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BSqVcGFbMTn"
      },
      "source": [
        "## Exercise 1: Image Classification (Google Colab)\n",
        "\n",
        "In this exercise, we will build a simple image classification model using a neural network to classify biomedical images into different categories. We will use the MedMNIST dataset for this purpose.\n",
        "\n",
        "**NOTE:** In the menubar, go to `Runtime > Change runtime type` and select `T4 GPU` for the **entirety of this workshop**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maQFLaRTbMTo",
        "outputId": "81bf4b0c-2e20-46d1-d160-69560e07fda8"
      },
      "outputs": [],
      "source": [
        "# import some common libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Download the dataset\n",
        "\n",
        "The first step is to download the dataset. Here, we will download the pathmnist subset of the MedMNIST dataset and save it to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the url\n",
        "pathmnist_url = \"https://zenodo.org/records/10519652/files/pathmnist.npz?download=1\"\n",
        "\n",
        "# import necessary libraries\n",
        "import os, urllib\n",
        "\n",
        "# download file from a url\n",
        "if not os.path.exists(\"pathmnist.npz\"):\n",
        "    urllib.request.urlretrieve(pathmnist_url, \"pathmnist.npz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Load the dataset in to memory\n",
        "\n",
        "Now that the dataset is available, we will load it into memory as train, test, and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the downloaded dataset\n",
        "data = np.load(\"pathmnist.npz\")\n",
        "print(\"Keys available in the data:\\n\", data.files)\n",
        "\n",
        "# split data into train, validation, and test sets and corresponding labels\n",
        "train_images, train_labels = data[\"train_images\"], data[\"train_labels\"]\n",
        "val_images, val_labels = data[\"val_images\"], data[\"val_labels\"]\n",
        "test_images, test_labels = data[\"test_images\"], data[\"test_labels\"]\n",
        "\n",
        "# Normalize the images to have pixel values between 0 and 1\n",
        "train_images = train_images / 255.0\n",
        "val_images = val_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "print(\"Shapes of the data:\")\n",
        "print(\"Train images shape:\", train_images.shape)\n",
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Val images shape:\", val_images.shape)\n",
        "print(\"Val labels shape:\", val_labels.shape)\n",
        "print(\"Test images shape:\", test_images.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)\n",
        "\n",
        "# also see how many classes are there in the dataset\n",
        "print(\"Number of classes in the dataset:\", len(np.unique(train_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Visualize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's plot some of the images along with their labels\n",
        "fig, axes = plt.subplots(1, 10, figsize=(10, 3))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(train_images[i])\n",
        "    ax.set_title(train_labels[i])\n",
        "    ax.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRA0XE9veEmw"
      },
      "source": [
        "### 4. Prepare data for training\n",
        "\n",
        "For training, data needs to be fed into the neural network in batches. The process shown here is for Tensorlow/Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import tensorflow libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert the labels to one-hot encoded format\n",
        "train_labels = to_categorical(train_labels)\n",
        "val_labels = to_categorical(val_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMQW2ls1eMAl"
      },
      "outputs": [],
      "source": [
        "# Create a data generator object that will be later used to generate batches of (optionally, augmented) images \n",
        "train_datagen = ImageDataGenerator() # No augmentation\n",
        "val_datagen = ImageDataGenerator() # No augmentation\n",
        "test_datagen = ImageDataGenerator() # No augmentation\n",
        "\n",
        "train_generator = train_datagen.flow(train_images, train_labels, batch_size=128)\n",
        "val_generator = val_datagen.flow(val_images, val_labels, batch_size=128)\n",
        "test_generator = test_datagen.flow(test_images, test_labels, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Build a neural network model for classification\n",
        "\n",
        "We will build a simple neural network model using Keras for image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "My7SrDM6iEzs",
        "outputId": "64a2f632-14f3-45bd-ddb1-e3e353315589"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 3)), \n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(9, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Train the model\n",
        "\n",
        "You can train for 20-50 epochs on a GPU inside of 5 minutes. Training for 5 epochs will take about 5-10 minutes when using a CPU kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tav-OJeLidro",
        "outputId": "469c655b-1517-4328-8c06-ea2baf0b6b13"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "YvwQ8nmLi_Ao",
        "outputId": "63e1f066-b0b1-475e-f9ea-8f36c24fc1e6"
      },
      "outputs": [],
      "source": [
        "# Let's visualize the training and validation accuracy and loss over the epochs.\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Evaluate the results\n",
        "\n",
        "We will plot a confusion matrix for the test dataset and calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# predict the test labels\n",
        "test_predictions = model.predict(test_images)\n",
        "\n",
        "# get the predicted labels\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# get the true labels\n",
        "test_true = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# print the accuracy\n",
        "print(\"Accuracy on test set:\", np.mean(test_predictions == test_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the confusion matrix\n",
        "cm = confusion_matrix(test_true, test_predictions)\n",
        "\n",
        "# plot the confusion matrix\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Using Fiji and DeepImageJ\n",
        "\n",
        "In this exercise, we will learn how to use Fiji (ImageJ) and deep learning models using plugins like `DeepImageJ` and model repositories such as `Bioimage.io`.\n",
        "\n",
        "Please follow the **instructions in the Workshop slides** to complete this exercise.\n",
        "\n",
        "You'll learn:\n",
        "- How to download and run Fiji (ImageJ)\n",
        "- How to install and use DeepImageJ plugin\n",
        "- How to use Bioimage.io model repository to download pre-trained deep learning models\n",
        "- How to run deep learning models to your images\n",
        "\n",
        "**Author's note** (read after the workshop): There are many pros and cons of using Fiji and plugins such as DeepImageJ. This is a quick way to get started with deep learning in bioimaging without writing any code. This is also a good way to stay up-to-date on popular tasks and models in the BioImaging community. However, you will soon realize that many of these models become outdated and incompatible (because of various software and environment that end users use) quickly and might be overly-complicated for your need because they try to generalize to a larger audience. The last point is especially true for open source deep-learning tools such as ZeroCostDL4Mic (covered later in the workshop). As you will learn in this workshop, it is possible to write simple, clean and effective Python code for your own project using libraries like TensorFlow, PyTorch, etc. to carry out similar tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Lmr75FlEvE"
      },
      "source": [
        "## Exercise 3: Image Segmentation with U-Net (Google Colab)\n",
        "\n",
        "In this exercise, we will build a U-Net model for image segmentation using the Kvasir-SEG dataset, which contains gastrointestinal polyp images and their corresponding segmentation masks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV0d6uQOlNcP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import cv2\n",
        "import urllib\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RlLomgQlWAh"
      },
      "source": [
        "### Download and extract the dataset\n",
        "\n",
        "We will use the Kvasir-SEG dataset for this exercise. Let's download and extract the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vgBjdeblZfz"
      },
      "outputs": [],
      "source": [
        "# Download the Kvasir-SEG dataset to local storage\n",
        "url = \"https://datasets.simula.no/downloads/kvasir-seg.zip\"\n",
        "zip_path = \"Kvasir-SEG.zip\"\n",
        "data_path = \"\"\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cRyeD3X6RaJ"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Let's load the images and masks from the extracted dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h3Fghxmuz1q",
        "outputId": "d4c3ffb8-ede1-4a45-e0a2-64f8c3b20184"
      },
      "outputs": [],
      "source": [
        "# define paths for images and masks\n",
        "image_dir = os.path.join(data_path, 'Kvasir-SEG/images')\n",
        "mask_dir = os.path.join(data_path, 'Kvasir-SEG/masks')\n",
        "\n",
        "# create empty lists to store images and masks\n",
        "images = []\n",
        "masks = []\n",
        "\n",
        "# loop through each file in the images and masks directories, read the images and masks, and store them in the lists\n",
        "for image_name in os.listdir(image_dir):\n",
        "    \n",
        "    image_path = os.path.join(image_dir, image_name)\n",
        "    mask_path = os.path.join(mask_dir, image_name)\n",
        "\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    image = cv2.resize(image, (64, 64))\n",
        "    mask = cv2.resize(mask, (64, 64))\n",
        "\n",
        "    images.append(image)\n",
        "    masks.append(mask)\n",
        "\n",
        "# convert the lists to numpy arrays\n",
        "images = np.array(images)\n",
        "masks = np.array(masks)\n",
        "\n",
        "# Normalize images\n",
        "images = images / 255.0\n",
        "masks = masks / 255.0\n",
        "masks = masks.reshape(-1, 64, 64, 1) # add an extra dimension for the channel\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes to ensure they are correct\n",
        "print(f'Train images shape: {train_images.shape}')\n",
        "print(f'Validation images shape: {val_images.shape}')\n",
        "print(f'Train masks shape: {train_masks.shape}')\n",
        "print(f'Validation masks shape: {val_masks.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE2jrbkZ6b13"
      },
      "source": [
        "### Build the U-Net model\n",
        "\n",
        "We will build a U-Net model for image segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kVqhubEpvcDn",
        "outputId": "639902ed-0fca-4234-dd5f-4e09e97a3b97"
      },
      "outputs": [],
      "source": [
        "def unet_model(input_size=(64, 64, 3)):\n",
        "    \n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "\n",
        "    # Decoder\n",
        "    up5 = Conv2D(128, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(drop4))\n",
        "    merge5 = concatenate([conv3, up5], axis=3)\n",
        "    conv5 = Conv2D(128, 3, activation='relu', padding='same')(merge5)\n",
        "    conv5 = Conv2D(128, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    up6 = Conv2D(64, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv5))\n",
        "    merge6 = concatenate([conv2, up6], axis=3)\n",
        "    conv6 = Conv2D(64, 3, activation='relu', padding='same')(merge6)\n",
        "    conv6 = Conv2D(64, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = Conv2D(32, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv6))\n",
        "    merge7 = concatenate([conv1, up7], axis=3)\n",
        "    conv7 = Conv2D(32, 3, activation='relu', padding='same')(merge7)\n",
        "    conv7 = Conv2D(32, 3, activation='relu', padding='same')(conv7)\n",
        "    \n",
        "    conv8 = Conv2D(1, 1, activation='sigmoid')(conv7)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv8)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = unet_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyEvJPNh6sRZ"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Now, we will train the model using the training data. Training for 25 epochs would have taken about an hour on a CPU kernel, hence we are using a **GPU kernel**. On a GPU kernel, this takes about a minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyKrpqJevobR",
        "outputId": "e8736c9c-1f14-40e0-f53d-ca7292c28d59"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_images, train_masks,\n",
        "    epochs=25,\n",
        "    batch_size=32,\n",
        "    validation_data=(val_images, val_masks)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebg54pgq61c4"
      },
      "source": [
        "### Evaluate the results\n",
        "\n",
        "Finally, we will evaluate the model's performance on the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "KMAUHXQExWEl",
        "outputId": "ec0f445b-a8f3-4670-9ff7-2d4253104a91"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTM6k8-t6_Kb"
      },
      "source": [
        "#### Visualize segmentation results\n",
        "\n",
        "Let's visualize some example segmentation results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pL9DdINgxa06",
        "outputId": "77c9c6f6-beb9-4a37-be1e-8875066970c3"
      },
      "outputs": [],
      "source": [
        "def display_results(model, images, labels, num_samples=10):\n",
        "    predictions = model.predict(images)\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(6, 18))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        axes[i, 0].imshow(images[i].reshape(64, 64, 3))\n",
        "        axes[i, 0].set_title('Input Image')\n",
        "        axes[i, 1].imshow(labels[i].reshape(64, 64), cmap='gray')\n",
        "        axes[i, 1].set_title('True Mask')\n",
        "        axes[i, 2].imshow(predictions[i].reshape(64, 64), cmap='gray')\n",
        "        axes[i, 2].set_title('Predicted Mask')\n",
        "        for ax in axes[i]:\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "display_results(model, val_images, val_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Optional] Exercise 4: Denoising using CARE (via ZeroCostDL4Mic)\n",
        "\n",
        "**NOTE:** You should have the Denoising_Dataset folder already uploaded to your Google Drive (see pre-workshop preparation section at the top).\n",
        "\n",
        "In this exercise, we will use the CARE (Content-Aware Image Restoration) neural network to denoise biomedical images. We will follow the Notebook implementation of ZeroCostDL4Mic directly, a platform that provides free access to deep learning models for bioimaging.\n",
        "\n",
        "1. Please click [here](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/CARE_2D_ZeroCostDL4Mic.ipynb) to open the ZeroCostDL4Mic notebook in a new Google Colab window.\n",
        "2. Delete the current Google Colab session.  \n",
        "Go to `Runtime > Manage Sessions` and delete the current session.  \n",
        "This is because you can only run one notebook at a time on the free tier.\n",
        "3. Create a copy of the ZeroCostDL4Mic notebook.  \n",
        "`File > Save a copy in Drive`.\n",
        "4. Connect to the `T4 GPU` runtime in the ZeroCostDL4Mic notebook. Not in this notebook. Training for 50 epochs takes about 2 minutes on a GPU kernel.\n",
        "5. Follow the instructions in the ZeroCostDL4Mic notebook. Just run all the cells one by one starting from Section 1.1. You'll need to change a few things along the way as mentioned below.\n",
        "\n",
        "### Things to change/enter in the notebook:\n",
        "\n",
        "You might need to adjust some of these based on where you uploaded the dataset in your Google Drive. Leave all other settings as they are in the notebook.\n",
        "\n",
        "- **Section 3.1**  \n",
        "Training_source: `/content/gdrive/MyDrive/Denoising_Dataset/Training/low`  \n",
        "Training_target: `/content/gdrive/MyDrive/Denoising_Dataset/Training/high`  \n",
        "model_name: `denoising_model`  \n",
        "model_path: `/content/gdrive/MyDrive/Denoising_Dataset/Results`  \n",
        "number_of_epochs: `5` you can increase/decrease this number depending on whether or not you are connected to a GPU kernel and/or if you have time`  \n",
        "\n",
        "- **Section 4.2** (Important)  \n",
        "Change lr to learning_rate in this line  \n",
        "`writer.writerow([history.history['loss'][i], history.history['val_loss'][i], history.history['learning_rate'][i]])`  \n",
        "\n",
        "- **Section 5.2**  \n",
        "Source_QC_folder: `/content/gdrive/MyDrive/Denoising_Dataset/test/Low`  \n",
        "Target_QC_folder: `/content/gdrive/MyDrive/Denoising_Dataset/test/High`  \n",
        "\n",
        "- **Section 6.1**  \n",
        "Data_folder: `/content/gdrive/MyDrive/Denoising_Dataset/test/Low`  \n",
        "Result_folder: `/content/gdrive/MyDrive/Denoising_Dataset/Results`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
